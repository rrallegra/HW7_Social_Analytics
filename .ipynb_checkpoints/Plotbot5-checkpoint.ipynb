{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import tweepy, time, json\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime, re, os, pytz\n",
    "\n",
    "# Setting timezone to EST\n",
    "timezone = pytz.timezone(\"US/Eastern\")\n",
    "\n",
    "# Create sentiment analyzer object\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Twitter API Keys\n",
    "consumer_key = \"Ed4RNulN1lp7AbOooHa9STCoU\"\n",
    "consumer_secret = \"P7cUJlmJZq0VaCY0Jg7COliwQqzK0qYEyUF9Y0idx4ujb3ZlW5\"\n",
    "access_token = \"839621358724198402-dzdOsx2WWHrSuBwyNUiqSEnTivHozAZ\"\n",
    "access_token_secret = \"dCZ80uNRbFDjxdU2EckmNiSckdoATach6Q8zb7YYYE5ER\"\n",
    "\n",
    "dt = datetime.datetime.today().strftime('%m/%d/%y')\n",
    "mth = datetime.datetime.today().strftime('%m-%y')\n",
    "analyzedAccts_lst, pendingAnalysis_lst, analysisRequestedFor_lst = [], [], []\n",
    "analyzedAccWithTime = {}\n",
    "\n",
    "# Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "myUsrDtls = api.me()\n",
    "myScreenName = \"@\" + myUsrDtls.screen_name\n",
    "\n",
    "# Function to perform sentiment analysis of a specific user's latest 500 tweets \n",
    "# & reply back to request tweet with the plot.\n",
    "\n",
    "def analyzeUserTweets(trgtUsrInfo):\n",
    "\n",
    "    compound_list, twtsAgo_list, screenName_list = [], [], []\n",
    "    tweetsAgo = 0\n",
    "\n",
    "    try:\n",
    "        for page in tweepy.Cursor(api.user_timeline, id=trgtUsrInfo[2], wait_on_rate_limit=True, wait_on_rate_limit_notify=True).pages(25):\n",
    "            for tweet in page:\n",
    "                tweetInfo = json.dumps(tweet._json, indent=3)\n",
    "                tweetInfo = json.loads(tweetInfo)\n",
    "                target_string = tweetInfo['text']\n",
    "                sentmt = analyzer.polarity_scores(target_string)\n",
    "                compound_list.append(sentmt['compound'])\n",
    "                screenName_list.append(tweetInfo['user']['screen_name'])\n",
    "                twtsAgo_list.append(tweetsAgo)\n",
    "                tweetsAgo = tweetsAgo + 1\n",
    "        \n",
    "        # Create dataframe to plot the graph\n",
    "        df = pd.DataFrame({'Tweets Ago':twtsAgo_list,'Compound Sentiments':compound_list,'ScreenName':screenName_list})\n",
    "        overallSentiment = df['Compound Sentiments'].mean()\n",
    "\n",
    "        # Plot the graph, save it as an image and tweet it back onto the original tweet\n",
    "        g = plt.plot(df['Tweets Ago'],df['Compound Sentiments'],marker='o',markersize=10)\n",
    "        g = plt.gca()\n",
    "        g.invert_xaxis()\n",
    "        plt.xlabel('Tweets Ago')\n",
    "        plt.ylabel('Tweet Polarity')\n",
    "        plt.title('Vedar Sentiment Analysis of latest 500 tweets ('+dt+')')\n",
    "        plt.legend(title='Tweets',bbox_to_anchor=(1, 1), loc='upper left', ncol=1,labels='@'+df['ScreenName'])\n",
    "        fileName = 'SentimentAnalysis_'+trgtUsrInfo[2][1:]+'.png'\n",
    "        plt.savefig(fileName, bbox_inches='tight')\n",
    "        api.update_with_media(fileName, trgtUsrInfo[1]+\" See the tweet sentiment analysis of \"+trgtUsrInfo[2]+\" you requested!\\nOverall Sentiment: \"+str(round(overallSentiment,2)), in_reply_to_status_id=trgtUsrInfo[0])\n",
    "        plt.gcf().clear()\n",
    "        os.remove(fileName)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Create function to get tweets requesting for analysis\n",
    "# Add the tweet data (tweet id, requester & target account) into a 2 lists: \n",
    "# i).  All requests \n",
    "# ii). Pending requests\n",
    "\n",
    "def getLatestRequests():\n",
    "\n",
    "    global analysisRequestedFor_lst\n",
    "    global pendingAnalysis_lst\n",
    "\n",
    "    try:\n",
    "        for tweet in tweepy.Cursor(api.mentions_timeline, count=20, result_type=\"recent\", include_entities=True, lang=\"en\", wait_on_rate_limit=True, wait_on_rate_limit_notify=True).items():\n",
    "            tweetInfo = json.dumps(tweet._json, indent=3)\n",
    "            tweetInfo = json.loads(tweetInfo)\n",
    "            twtText = tweetInfo[\"text\"]\n",
    "            if (myScreenName in twtText) and ((\"Analyze\" in twtText) or (\"analyze\" in twtText)):\n",
    "                userInfo = []\n",
    "                twt_id = tweetInfo[\"id\"]\n",
    "                twted_user = \"@\"+tweetInfo['user']['screen_name']\n",
    "                trgted_user = \"@\"+tweetInfo['entities']['user_mentions'][1]['screen_name']\n",
    "                userInfo.append(twt_id)\n",
    "                userInfo.append(twted_user)\n",
    "                userInfo.append(trgted_user)\n",
    "                if userInfo in analysisRequestedFor_lst:\n",
    "                    pass\n",
    "                else:\n",
    "                    analysisRequestedFor_lst.append(userInfo)\n",
    "                    pendingAnalysis_lst.append(userInfo)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Infinite loop to keep checking on latest requests to analyze sentiment every 15 minutes\n",
    "while(True):\n",
    "    getLatestRequests()\n",
    "    if (len(pendingAnalysis_lst) == 0):\n",
    "        pass\n",
    "    else:\n",
    "        twtDtls = pendingAnalysis_lst[0]\n",
    "        acctToAnalyze = twtDtls[2] + \"_\" + mth\n",
    "        isAnalyzed = False\n",
    "        if acctToAnalyze in analyzedAccts_lst:\n",
    "            try: \n",
    "                api.update_status(twtDtls[1]+\" Analyzed \"+twtDtls[2]+ \" on \"+ analyzedAccWithTime[acctToAnalyze] + \". Sorry now: \"+ (datetime.datetime.utcfromtimestamp(time.time()).replace(tzinfo=pytz.utc).astimezone(timezone)).strftime('%m-%d-%y %H:%M:%S')+\" \"+(datetime.datetime.utcfromtimestamp(time.time()).replace(tzinfo=pytz.utc).astimezone(timezone)).tzname() +\". Check homepage for report or try next month.\", in_reply_to_status_id=twtDtls[0])\n",
    "                isAnalyzed = True\n",
    "            except:\n",
    "                isAnalyzed = False\n",
    "        else:\n",
    "            isAnalyzed = analyzeUserTweets(twtDtls)\n",
    "\n",
    "        if isAnalyzed:\n",
    "            analyzedAccts_lst.append(acctToAnalyze)\n",
    "            analyzedAccWithTime[acctToAnalyze] = (datetime.datetime.utcfromtimestamp(time.time()).replace(tzinfo=pytz.utc).astimezone(timezone)).strftime('%m-%d-%y')\n",
    "            del pendingAnalysis_lst[0]\n",
    "    \n",
    "    time.sleep(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
